{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd6f1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved audio_chunks\\chunk_1.wav\n",
      "Saved audio_chunks\\chunk_2.wav\n",
      "Saved audio_chunks\\chunk_3.wav\n",
      "Saved audio_chunks\\chunk_4.wav\n",
      "Saved audio_chunks\\chunk_5.wav\n",
      "Saved audio_chunks\\chunk_6.wav\n",
      "Saved audio_chunks\\chunk_7.wav\n",
      "Saved audio_chunks\\chunk_8.wav\n",
      "Saved audio_chunks\\chunk_9.wav\n",
      "Saved audio_chunks\\chunk_10.wav\n",
      "Saved audio_chunks\\chunk_11.wav\n",
      "Audio file has been split into 11 chunks and saved in 'audio_chunks'.\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def split_audio(file_path, chunk_length_ms, output_dir=\"audio_chunks\"):\n",
    "    \"\"\"\n",
    "    Splits an audio file into chunks of the specified length and saves them in the output directory.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input audio file.\n",
    "        chunk_length_ms (int): Length of each chunk in milliseconds.\n",
    "        output_dir (str): Directory where the chunks will be saved.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "    # Calculate the number of chunks\n",
    "    total_length = len(audio)\n",
    "    num_chunks = total_length // chunk_length_ms\n",
    "\n",
    "    # Split and save chunks\n",
    "    for i in range(num_chunks + 1):\n",
    "        start_time = i * chunk_length_ms\n",
    "        end_time = min(start_time + chunk_length_ms, total_length)\n",
    "        chunk = audio[start_time:end_time]\n",
    "\n",
    "        # Define the filename for the chunk\n",
    "        chunk_filename = os.path.join(output_dir, f\"chunk_{i + 1}.wav\")\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "        print(f\"Saved {chunk_filename}\")\n",
    "\n",
    "    print(f\"Audio file has been split into {num_chunks + 1} chunks and saved in '{output_dir}'.\")\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"output_audio.mp3\"  # Replace with your audio file path\n",
    "chunk_duration = 30000  # 30 seconds in milliseconds\n",
    "split_audio(audio_file, chunk_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1e726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk_1.wav: we should know how to handle not only how to handle success how to handle the failures particularly you are in the management environment water it I want the young people to understand how to manage the failure because any task you do you have to come across problem problem should not become the captain of the\n",
      "Processed chunk_10.wav: great human being sometime they will become better than you but better than the teacher so that opportunity to have I will have great you I will continuously acquire knowledge how I will do hard work I will Persevere and succeed\n",
      "Could not understand audio: audio_chunks\\chunk_11.wav\n",
      "Processed chunk_11.wav: \n",
      "Processed chunk_2.wav: individual or a project chief the project chief should become the captain of the problems and defeat the problem and succeed learning use creativity creativity leads to thinking thinking provides knowledge makes you great\n",
      "Processed chunk_3.wav: the those who directed Imagine The Impossible are the ones who break all the human limitations in every field of human Endeavour whether science medicine sports are the technology the names of the people Imagine The Impossible are engraved in our history by\n",
      "Processed chunk_4.wav: Subway imagination by breaking their limits of their imagination change the change in the world you take CV Raman you take Newton you take Einstein you take Chandrashekhar the by breaking the limits of their imagination that change the world will not be discoverers if you want to be innovators I am going to give you what type of\n",
      "Processed chunk_5.wav: what type of characteristic you must have invention and discoveries have eliminated from creative Minds that have been constantly working and imaging the outcome that telephone he was imagined outcome imagine the outcome in the mind and constant effort all the forces of the universe work for that inspired my dear by leading to\n",
      "Processed chunk_6.wav: I would like to hear from you a few tips for the upcoming generation to succeed in life well succeeding life I have already told you you have to do 40 ok number one great I will have great knowledge\n",
      "Processed chunk_7.wav: how I will do hard work I will do hard work I will perceive here so much do you have money books if service president\n",
      "Processed chunk_8.wav: you know I had I had a teacher in whenever the young boy 10 years boy what time the Second World War going on at the time I used to see in my class 5th class teacher in science teacher he entered the classroom and used to see the radiation of knowledge from radiation of knowledge of purity of life\n",
      "Processed chunk_9.wav: and his the way he taught I my dream is God shaped what should be my way of life he is a person the teacher gave me the vision of my life when I was young fellow now teacher has got a fantastic opportunity to grow Minds to enter the minds and give the dreams to the young people and nurture the teams with them and\n",
      "Transcriptions saved to transcriptions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def transcribe_audio_to_text(audio_path):\n",
    "    \"\"\"\n",
    "    Transcribe audio file to text using Google's Web Speech API.\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file to be transcribed.\n",
    "    Returns:\n",
    "        str: Transcription of the audio.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Convert audio to WAV if it's not already in WAV format\n",
    "    if not audio_path.endswith(\".wav\"):\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        audio_path = audio_path.replace(os.path.splitext(audio_path)[1], \".wav\")\n",
    "        audio.export(audio_path, format=\"wav\")\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "    # Use Google's Web Speech API to transcribe\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(f\"Could not understand audio: {audio_path}\")\n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        print(\"Could not request results from Google Speech Recognition service\")\n",
    "        return \"\"\n",
    "\n",
    "def create_transcriptions_csv(audio_dir, output_csv=\"transcriptions.csv\"):\n",
    "    \"\"\"\n",
    "    Create a CSV file with transcriptions of all audio files in the given directory.\n",
    "    Args:\n",
    "        audio_dir (str): Directory containing the audio files to be transcribed.\n",
    "        output_csv (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Open CSV file for writing\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"audio_file\", \"transcription\"])\n",
    "\n",
    "        # Iterate over all audio files in the directory\n",
    "        for audio_file in os.listdir(audio_dir):\n",
    "            if audio_file.endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "                audio_path = os.path.join(audio_dir, audio_file)\n",
    "                transcription = transcribe_audio_to_text(audio_path)\n",
    "\n",
    "                # Write audio file name and transcription to the CSV file\n",
    "                writer.writerow([audio_file, transcription])\n",
    "                print(f\"Processed {audio_file}: {transcription}\")\n",
    "\n",
    "    print(f\"Transcriptions saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "audio_directory = \"audio_chunks\"  # Directory containing your audio files\n",
    "create_transcriptions_csv(audio_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88abe4e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'espnet2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mespnet2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mespnet2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts_train\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text2Speech\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mespnet2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TTSTask\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'espnet2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import espnet2\n",
    "from espnet2.bin.tts_train import Text2Speech\n",
    "from espnet2.tasks.tts import TTSTask\n",
    "from espnet2.train import Trainer\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "\n",
    "def prepare_training_data(audio_dir, metadata_file):\n",
    "    \"\"\"\n",
    "    Prepare dataset for training.\n",
    "    Args:\n",
    "        audio_dir (str): Directory containing audio chunks.\n",
    "        metadata_file (str): Path to the metadata file with transcriptions.\n",
    "    Returns:\n",
    "        dataset (list of tuples): List containing (audio_path, transcription).\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            audio_file, transcription = line.strip().split(',')\n",
    "            audio_path = os.path.join(audio_dir, audio_file)\n",
    "            if os.path.exists(audio_path):\n",
    "                dataset.append((audio_path, transcription))\n",
    "    return dataset\n",
    "\n",
    "def fine_tune_tts(pretrained_model, dataset, output_dir=\"fine_tuned_model\", epochs=5, batch_size=8):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained TTS model using custom audio and transcriptions.\n",
    "    Args:\n",
    "        pretrained_model (str): Path or identifier for the pre-trained model.\n",
    "        dataset (list): List of (audio_path, transcription).\n",
    "        output_dir (str): Directory to save the fine-tuned model.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Size of each batch for training.\n",
    "    \"\"\"\n",
    "    downloader = ModelDownloader()\n",
    "    model = downloader.download_and_unpack(pretrained_model)\n",
    "\n",
    "    train_data = TTSTask.build_data_loader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(model=model, output_dir=output_dir)\n",
    "\n",
    "    # Start Fine-Tuning\n",
    "    trainer.run(train_data, epochs=epochs)\n",
    "\n",
    "# Example usage\n",
    "audio_directory = \"audio_chunks\"  # Directory with your audio chunks\n",
    "metadata_csv = \"metadata.csv\"     # Path to your metadata file\n",
    "dataset = prepare_training_data(audio_directory, metadata_csv)\n",
    "\n",
    "# Specify the pre-trained model name you want to fine-tune\n",
    "pretrained_tts_model = \"espnet/kan-bayashi_ljspeech_tacotron2\"\n",
    "\n",
    "# Fine-tune the TTS model\n",
    "fine_tune_tts(pretrained_tts_model, dataset, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a489b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow==2.12.0)\n",
      "  Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.7.0)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting numpy<1.24,>=1.22 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading grpcio-1.66.2-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.38.4)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.34-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading ml_dtypes-0.5.0-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.33-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.31-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.30-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.10.1)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.29.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nihanth\\new folder\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "   ---------------------------------------- 0.0/272.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/272.9 MB 5.6 MB/s eta 0:00:49\n",
      "    --------------------------------------- 5.0/272.9 MB 17.7 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 7.3/272.9 MB 13.7 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 11.3/272.9 MB 15.7 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 14.2/272.9 MB 14.8 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 17.8/272.9 MB 15.2 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 20.2/272.9 MB 14.5 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 22.5/272.9 MB 14.0 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 24.6/272.9 MB 13.7 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 27.0/272.9 MB 13.4 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 29.6/272.9 MB 13.1 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 32.0/272.9 MB 13.0 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 34.6/272.9 MB 12.9 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 37.2/272.9 MB 12.9 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 39.8/272.9 MB 12.8 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 42.7/272.9 MB 12.8 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 45.4/272.9 MB 12.8 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 48.0/272.9 MB 12.7 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 50.6/272.9 MB 12.7 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 53.5/272.9 MB 12.8 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 56.1/272.9 MB 12.7 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 58.7/272.9 MB 12.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 61.6/272.9 MB 12.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 64.2/272.9 MB 12.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 67.1/272.9 MB 12.7 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 69.7/272.9 MB 12.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 72.4/272.9 MB 12.7 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 75.2/272.9 MB 12.7 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 77.9/272.9 MB 12.7 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 80.7/272.9 MB 12.7 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 82.8/272.9 MB 12.6 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 85.5/272.9 MB 12.6 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 88.3/272.9 MB 12.6 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 91.0/272.9 MB 12.6 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 93.1/272.9 MB 12.5 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 95.7/272.9 MB 12.5 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 98.3/272.9 MB 12.5 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 101.2/272.9 MB 12.6 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 104.1/272.9 MB 12.6 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 106.7/272.9 MB 12.6 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 109.3/272.9 MB 12.6 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 111.9/272.9 MB 12.6 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 114.6/272.9 MB 12.6 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 117.4/272.9 MB 12.6 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 120.1/272.9 MB 12.6 MB/s eta 0:00:13\n",
      "   ----------------- --------------------- 122.7/272.9 MB 12.6 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 125.6/272.9 MB 12.6 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 128.5/272.9 MB 12.6 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 131.3/272.9 MB 12.6 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 134.2/272.9 MB 12.6 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 137.1/272.9 MB 12.6 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 140.0/272.9 MB 12.7 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 142.9/272.9 MB 12.7 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 145.8/272.9 MB 12.7 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 148.6/272.9 MB 12.7 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 151.8/272.9 MB 12.7 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 154.7/272.9 MB 12.8 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 157.8/272.9 MB 12.8 MB/s eta 0:00:10\n",
      "   ----------------------- --------------- 161.0/272.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 164.1/272.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 167.0/272.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 169.9/272.9 MB 12.9 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 173.0/272.9 MB 12.9 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 176.4/272.9 MB 12.9 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 179.8/272.9 MB 13.0 MB/s eta 0:00:08\n",
      "   -------------------------- ------------ 183.0/272.9 MB 13.0 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 186.4/272.9 MB 13.0 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 189.8/272.9 MB 13.1 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 193.5/272.9 MB 13.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 196.3/272.9 MB 13.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 199.5/272.9 MB 13.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 201.9/272.9 MB 13.2 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 204.7/272.9 MB 13.1 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 207.4/272.9 MB 13.1 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 210.2/272.9 MB 13.1 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 213.1/272.9 MB 13.1 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 216.0/272.9 MB 13.1 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 219.2/272.9 MB 13.1 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 222.0/272.9 MB 13.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 225.2/272.9 MB 13.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 228.6/272.9 MB 13.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 231.7/272.9 MB 13.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 234.9/272.9 MB 13.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 238.3/272.9 MB 13.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 240.9/272.9 MB 13.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 244.1/272.9 MB 13.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 247.5/272.9 MB 13.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 250.9/272.9 MB 13.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 254.3/272.9 MB 13.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 257.9/272.9 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 261.1/272.9 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 264.0/272.9 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 265.8/272.9 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  267.6/272.9 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  269.5/272.9 MB 13.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  271.6/272.9 MB 13.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 272.9/272.9 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.66.2-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.8/26.4 MB 9.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.9/26.4 MB 9.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.8/26.4 MB 8.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.6/26.4 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.7/26.4 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.1/26.4 MB 9.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 14.2/26.4 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 16.3/26.4 MB 9.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 18.4/26.4 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.4/26.4 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.4 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 9.2 MB/s eta 0:00:00\n",
      "Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.8/5.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 9.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading jaxlib-0.4.30-cp311-cp311-win_amd64.whl (51.9 MB)\n",
      "   ---------------------------------------- 0.0/51.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/51.9 MB 9.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 3.9/51.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 6.0/51.9 MB 9.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 8.4/51.9 MB 9.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 10.5/51.9 MB 9.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 12.6/51.9 MB 9.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 14.7/51.9 MB 9.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 16.8/51.9 MB 9.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 18.6/51.9 MB 9.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 20.7/51.9 MB 9.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 23.1/51.9 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 25.2/51.9 MB 9.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 27.5/51.9 MB 9.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 29.6/51.9 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 31.7/51.9 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 34.1/51.9 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 36.2/51.9 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 38.3/51.9 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 40.6/51.9 MB 10.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 42.2/51.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 44.6/51.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 46.9/51.9 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 49.3/51.9 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  51.9/51.9 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.9/51.9 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.0-cp311-cp311-win_amd64.whl (211 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, ml-dtypes, google-auth, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.0 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.35.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.66.2 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 libclang-18.1.1 ml-dtypes-0.5.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.4.0 protobuf-4.25.5 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b611ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TensorFlowTTS\n",
      "  Using cached TensorFlowTTS-1.8-py3-none-any.whl.metadata (24 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflowtts to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached TensorFlowTTS-1.6.1-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached TensorFlowTTS-1.6-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached TensorFlowTTS-1.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached TensorFlowTTS-0.11-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tensorflow-gpu>=2.3.1 (from TensorFlowTTS)\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [44 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 35, in __init__\n",
      "      parsed = parse_requirement(requirement_string)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 64, in parse_requirement\n",
      "      return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 82, in _parse_requirement\n",
      "      url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 126, in _parse_requirement_details\n",
      "      marker = _parse_requirement_marker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 147, in _parse_requirement_marker\n",
      "      tokenizer.raise_syntax_error(\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_tokenizer.py\", line 163, in raise_syntax_error\n",
      "      raise ParserSyntaxError(\n",
      "  setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\NIHANTH\\AppData\\Local\\Temp\\pip-install-ce6g5qu1\\tensorflow-gpu_d8adec0ba3f8413ba9f61f5556d52ce4\\setup.py\", line 40, in <module>\n",
      "      setuptools.setup()\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\__init__.py\", line 106, in setup\n",
      "      _install_setup_requires(attrs)\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\__init__.py\", line 77, in _install_setup_requires\n",
      "      dist.parse_config_files(ignore_option_errors=True)\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\dist.py\", line 910, in parse_config_files\n",
      "      self._finalize_requires()\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\dist.py\", line 607, in _finalize_requires\n",
      "      self._move_install_requirements_markers()\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\dist.py\", line 647, in _move_install_requirements_markers\n",
      "      inst_reqs = list(_reqs.parse(spec_inst_reqs))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\NIHANTH\\New folder\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 37, in __init__\n",
      "      raise InvalidRequirement(str(e)) from e\n",
      "  setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install TensorFlowTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b529a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/TensorSpeech/TensorFlowTTS.gitNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Cloning https://github.com/TensorSpeech/TensorFlowTTS.git to c:\\users\\nihanth\\appdata\\local\\temp\\pip-req-build-__ub7nxm\n",
      "  Resolved https://github.com/TensorSpeech/TensorFlowTTS.git to commit 136877136355c82d7ba474ceb7a8f133bd84767e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of tensorflowtts to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/TensorSpeech/TensorFlowTTS.git 'C:\\Users\\NIHANTH\\AppData\\Local\\Temp\\pip-req-build-__ub7nxm'\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.7.0 (from tensorflowtts) (from versions: 2.12.0)\n",
      "ERROR: No matching distribution found for tensorflow-gpu==2.7.0\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/TensorSpeech/TensorFlowTTS.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daef3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
